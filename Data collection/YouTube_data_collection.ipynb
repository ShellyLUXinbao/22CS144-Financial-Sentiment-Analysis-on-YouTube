{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: A list of 15 YouTube news channels is mannually created"
      ],
      "metadata": {
        "id": "oSbILBg7l0em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "Iu3cqMaVst8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "channels = [\n",
        "    {'channel_name': 'CNBC Television',\n",
        "     'channel_id': 'UCrp_UI8XtuYfpiqluWLD7Lw'},\n",
        "    {'channel_name': 'Yahoo Finance',\n",
        "     'channel_id': 'UCEAZeUIeJs0IjQiqTCdVSIg'},\n",
        "    {'channel_name': 'Bloomberg Television',\n",
        "     'channel_id': 'UCIALMKvObZNtJ6AmdCLP7Lg'},\n",
        "    {'channel_name': 'Financial Times',\n",
        "     'channel_id': 'UCoUxsWakJucWg46KW5RsvPw'},\n",
        "    {'channel_name': 'The Wall Street Journal',\n",
        "     'channel_id': 'UCK7tptUDHh-RYDsdxO1-5QQ'},\n",
        "    {'channel_name': 'Reuters',\n",
        "     'channel_id': 'UChqUTb7kYRX8-EiaN3XFrSQ'},\n",
        "    {'channel_name': 'The New York Times',\n",
        "     'channel_id': 'UCqnbDFdCpuN8CMEg0VuEBqA'},\n",
        "    {'channel_name': 'The Guardian',\n",
        "     'channel_id': 'UCHpw8xwDNhU9gdohEcJu4aA'},\n",
        "    {'channel_name': 'Fox News',\n",
        "     'channel_id': 'UCXIJgqnII2ZOINSWNOGFThA'},\n",
        "    {'channel_name': 'CNN',\n",
        "     'channel_id': 'UCupvZG-5ko_eiXAupbDfxWw'},\n",
        "    {'channel_name': 'NBC News',\n",
        "     'channel_id': 'UCeY0bbntWzzVIaj2z3QigXg'},\n",
        "    {'channel_name': 'ABC News',\n",
        "     'channel_id': 'UCBi2mrWuNuyYy4gbM6fU18Q'},\n",
        "    {'channel_name': 'CBS News',\n",
        "     'channel_id': 'UC8p1vwvWtl6T73JiExfWs1g'},\n",
        "    {'channel_name': 'BBC News',\n",
        "     'channel_id': 'UC16niRr50-MSBwiO3YDb3RA'},\n",
        "    {'channel_name': 'The Economist',\n",
        "     'channel_id': 'UC0p5jTq6Xx_DosDFxVXnWaQ'}\n",
        "]"
      ],
      "metadata": {
        "id": "waNS3W7lmV04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Within each channel, search videos that are stock-related"
      ],
      "metadata": {
        "id": "LZ6gEUDQmhA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube Data API\n",
        "from googleapiclient.discovery import build\n",
        "api_key = 'AIzaSyBEh4VNBKr6hzjjgTklbdprs5veavuPUMA'\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)"
      ],
      "metadata": {
        "id": "G2Th-JlrmyiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_ids(query, channel_id, published_after, published_before):\n",
        "  videos = []\n",
        "  next_page_token = None\n",
        "  while True:\n",
        "      video_request = youtube.search().list(\n",
        "          q=query,\n",
        "          part=\"id\",\n",
        "          channelId=channel_id,\n",
        "          maxResults=50,\n",
        "          pageToken=next_page_token,\n",
        "          publishedAfter=published_after,\n",
        "          publishedBefore=published_before,\n",
        "          type=\"video\",\n",
        "          videoDefinition=\"high\",\n",
        "          )\n",
        "      video_response = video_request.execute()\n",
        "      videos += video_response[\"items\"]\n",
        "      next_page_token = video_response.get(\"nextPageToken\")\n",
        "      if not next_page_token:\n",
        "          break\n",
        "  video_ids = [video[\"id\"].get(\"videoId\") for video in videos]\n",
        "  return video_ids\n",
        "\n",
        "\n",
        "channel_ids = [channel['channel_id'] for channel in channels]\n",
        "query = \"stock\"\n",
        "published_after = '2024-02-25T00:00:00Z'\n",
        "published_before = '2024-03-11T00:00:00Z'\n",
        "\n",
        "video_ids = []\n",
        "for cid in channel_ids:\n",
        "  current_video_ids = get_video_ids(query, cid, published_after, published_before)\n",
        "  video_ids += current_video_ids"
      ],
      "metadata": {
        "id": "0IrT7gfNmf7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# discard live videos\n",
        "def remove_live_videos(video_ids):\n",
        "  updated_video_ids = []\n",
        "  for videoId in video_ids:\n",
        "    response = youtube.videos().list(\n",
        "        part='liveStreamingDetails',\n",
        "        id=videoId\n",
        "        ).execute()\n",
        "    try:\n",
        "      live_streaming_details = response['items'][0]['liveStreamingDetails']\n",
        "    except:\n",
        "      updated_video_ids.append(videoId)\n",
        "\n",
        "  return updated_video_ids\n",
        "\n",
        "updated_video_ids = remove_live_videos(video_ids)"
      ],
      "metadata": {
        "id": "NbkJszsZnDdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Fetch video information and transcript"
      ],
      "metadata": {
        "id": "p2lh8lT1nPkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTubeTranscriptApi\n",
        "!pip install youtube_transcript_api\n",
        "from youtube_transcript_api import YouTubeTranscriptApi"
      ],
      "metadata": {
        "id": "qdUnuFfGnhVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_transcript(videoId):\n",
        "  try:\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(videoId)\n",
        "  except:\n",
        "    transcript = ''\n",
        "\n",
        "  final_transcript = ''\n",
        "  if len(transcript) > 10:\n",
        "    for caption in transcript:\n",
        "      final_transcript += caption['text']\n",
        "      final_transcript += ' '\n",
        "\n",
        "  return final_transcript\n",
        "\n",
        "\n",
        "def get_video_data(videoId):\n",
        "  response = youtube.videos().list(\n",
        "      part='snippet',\n",
        "      id=videoId\n",
        "  ).execute()\n",
        "  title = response['items'][0]['snippet']['title']\n",
        "  upload_date = response['items'][0]['snippet']['publishedAt']\n",
        "  upload_date = datetime.datetime.strptime(upload_date, '%Y-%m-%dT%H:%M:%S%z')\n",
        "  channel = response['items'][0]['snippet']['channelTitle']\n",
        "\n",
        "  transcript = get_video_transcript(videoId)\n",
        "\n",
        "  return title, channel, upload_date, transcript\n",
        "\n",
        "\n",
        "video_list = []\n",
        "video = {}\n",
        "for videoId in updated_video_ids:\n",
        "    title, channel, upload_date, transcript = get_video_data(videoId)\n",
        "    if transcript == '':\n",
        "      continue\n",
        "    video = {'Video ID': videoId, 'Title': title, 'Channel': channel, 'Upload date': upload_date, 'Transcipt': transcript}\n",
        "    video_list.append(video)"
      ],
      "metadata": {
        "id": "rKzBvZJonOin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Identify S&P 500 stocks from video titles"
      ],
      "metadata": {
        "id": "sLborlOeoBEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path change required\n",
        "sp500_df = pd.DataFrame(pd.read_excel('sp500.xlsx'))\n",
        "sp500_df.drop_duplicates(inplace=True)\n",
        "sp500_df"
      ],
      "metadata": {
        "id": "sn1hV8QPoUIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_stocks_from_titles(title):\n",
        "  identified_stocks = []\n",
        "  for i in range(len(sp500_df)):\n",
        "    company = sp500_df.iloc[i]['Company']\n",
        "    symbol = sp500_df.iloc[i]['Symbol']\n",
        "\n",
        "    # check if company in title\n",
        "    pattern = r'\\b{}\\b'.format(re.escape(company))\n",
        "    match = re.search(pattern, title)\n",
        "    if match:\n",
        "      identified_stocks.append(i)\n",
        "    # check if symbol in title given that company is not in title\n",
        "    else:\n",
        "      pattern = r'\\b{}\\b'.format(re.escape(symbol))\n",
        "      match = re.search(pattern, title)\n",
        "      if match:\n",
        "        identified_stocks.append(i)\n",
        "\n",
        "  return identified_stocks\n",
        "\n",
        "\n",
        "for video in video_list:\n",
        "  identified_stocks = identify_stocks_from_titles(video['Title'])\n",
        "  video['Stock_indices'] = identified_stocks"
      ],
      "metadata": {
        "id": "gOp6NfwYoMXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Save collected data"
      ],
      "metadata": {
        "id": "ZINs3hlGq4M0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(video_list).to_csv('youtube_collected_2024.csv')"
      ],
      "metadata": {
        "id": "mXNzXYsEsINv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}